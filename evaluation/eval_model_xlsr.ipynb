{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Performance of IPA Transcription Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "from eval_helpers import remove_diacritics\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TIMIT_PATH = \"kylelovesllms/timit_asr_ipa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit = load_dataset(HF_TIMIT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Load Baseline Model Wav2Vec2XLSR-53-espeak\n",
    "- HF Link: https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsr_processor = Wav2Vec2Processor.from_pretrained(\n",
    "    \"facebook/wav2vec2-xlsr-53-espeak-cv-ft\")\n",
    "xlsr_model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-xlsr-53-espeak-cv-ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep audio to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=xlsr_processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = xlsr_processor\n",
    "def prepare_dataset(batch):\n",
    "    # Each batch corresponds with a specific entry (either in train or test)\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        timit_ipa_labels_arr = batch[\"ipa_transcription\"]\n",
    "        ipa_transcription = \"\".join(timit_ipa_labels_arr)\n",
    "        batch[\"labels\"] = processor(ipa_transcription).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit = timit.map(prepare_dataset, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Apply Model (Get Transcriptions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_result(batch):\n",
    "  with torch.no_grad():\n",
    "    input_values = torch.tensor(batch[\"input_values\"], device=\"cpu\").unsqueeze(0)\n",
    "    logits = xlsr_model(input_values).logits\n",
    "\n",
    "  pred_ids = torch.argmax(logits, dim=-1)\n",
    "  batch[\"pred_str\"] = xlsr_processor.batch_decode(pred_ids)[0]\n",
    "  batch[\"text\"] = xlsr_processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_transcription(batch):\n",
    "    # Cleanup Wav2Vec2 Transcription\n",
    "    cleanedup_w2v2p2_transcription = remove_diacritics(batch[\"pred_str\"].split(\" \"))\n",
    "    transcription_as_str = \"\".join(cleanedup_w2v2p2_transcription)\n",
    "    batch[\"pred_str\"] = transcription_as_str\n",
    "\n",
    "    # Stringify the IPA transcription \n",
    "    batch[\"ipa_transcription\"] = \"\".join(batch[\"ipa_transcription\"])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model inference and sanitize transcriptions\n",
    "results = timit[\"test\"].map(map_to_result).map(sanitize_transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "print(f'Ground Truth: {results[0][\"ipa_transcription\"]}')\n",
    "print(f'Prediction:   {results[0][\"pred_str\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) Evaluate Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_metric = load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cer = cer_metric.compute(predictions=results[\"pred_str\"], references=results[\"ipa_transcription\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test CER: {:.3f}\".format(total_cer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
