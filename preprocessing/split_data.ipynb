{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data\n",
    "- Splits the TIMIT dataset into a `TRAIN`, `VALIDATION`, `TEST` dataset\n",
    "- Note: the TIMIT dataset already comes with a `TEST` folder so we will simply take a % of the `TEST` dataset for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Import Audio\n",
    "- Create Training Dataset\n",
    "- Create Test Dataset\n",
    "- Since TIMIT does not differentiate between Test and Validation, we will manually split the Test into Test and Validation as a downstream task to respect the original naming conventions of the TIMIT dataset\n",
    "\n",
    "### Motivation\n",
    "The motivation for creating this dataset is because the existing `timit_asr` Hugging Face datasets do not use the complete TIMIT `TEST` and `TRAIN` data available. \n",
    "\n",
    "In addition, we also add  speaker gender and duration of speech.\n",
    "\n",
    "### Linguistic Distinction in categories\n",
    "`SA` = \"Speaker Accent\"/Dialect or Shibboleth sentences designed to highlight dialect region differences\n",
    "`SX` = Phonetically Compact sentences designed to highlight pairs of phones of interest (i.e. voiced vs unvoiced velar stops) in specific phonetic contexts (i.e. coda position = at the end of a word)\n",
    "`SI` = Phonetically Diverse sentences designed to highlight many different phonemes and sentence types per speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Audio, DatasetDict, load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timit_metadata_extractors import get_transcription_detail, get_speech_duration, get_replace_ending, get_timit_path, get_speaker_info, get_sentence_info, get_text, get_ipa_transcription\n",
    "from timit_dataset_splitter import stratify_timt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TIMIT = \"../data/input_data/TIMIT-Database/TIMIT\"\n",
    "UPLOAD_TIMIT_BASE_NAME = \"kylelovesllms/timit_asr\"\n",
    "UPLOAD_TIMIT_IPA_NAME = \"kylelovesllms/timit_asr_ipa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_files(dir: str, file_type: str = \"wav\") -> dict[tuple[str, list[str]], tuple[str, list[str]]]:\n",
    "    \"\"\"\n",
    "    Walks through every directory in `dir` and returns all files which end in `file_type`\n",
    "    \"\"\"\n",
    "\n",
    "    audio_paths = []\n",
    "\n",
    "    # Walk through each directory\n",
    "    for dirpath, _dirnames, filenames in os.walk(dir):\n",
    "        # Check each file in the directory\n",
    "        for file_name in filenames:\n",
    "            # If the `file_type` matches\n",
    "            if file_name.endswith(file_type):\n",
    "                # Add that file\n",
    "                full_local_path = os.path.join(dirpath, file_name)\n",
    "                audio_paths.append(full_local_path)\n",
    "    return audio_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check that we get the appropriate local paths and task names\n",
    "paths = get_audio_files(\"../data/input_data/TIMIT-Database/TIMIT/TEST\")\n",
    "print(\"local paths\", paths[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(PATH_TO_TIMIT, \"TRAIN\")\n",
    "testvalidation_path = os.path.join(PATH_TO_TIMIT, \"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"audio\": [audio_path for audio_path in get_audio_files(train_path)]\n",
    "    }\n",
    ").cast_column(\"audio\", Audio())\n",
    "\n",
    "testvalidation_dataset = Dataset.from_dict(\n",
    "     {\n",
    "        \"audio\": [audio_path for audio_path in get_audio_files(testvalidation_path)]\n",
    "    }\n",
    ").cast_column(\"audio\", Audio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check to make sure we have valid audio files\n",
    "train_dataset[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Add Metadata\n",
    "Although not all of the metadata is needed for this specific project, perhaps others in the open-source community will find this metadata helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metadata(example):\n",
    "    \"\"\"\n",
    "    Adds transcriptions and metadata to the TIMIT dataset\n",
    "\n",
    "    Note: after using `example[\"audio\"]`, the `example[\"audio\"][\"path\"]` is \n",
    "    automatically set to None for security reasons\n",
    "    (see Github Issue: https://github.com/huggingface/datasets/issues/5190)\n",
    "    \"\"\"\n",
    "    wav_path = example[\"audio\"][\"path\"]\n",
    "\n",
    "    # Add Transcriptions\n",
    "    phn_file = get_replace_ending(wav_path, new_extension=\".phn\")\n",
    "    wrd_file = get_replace_ending(wav_path, new_extension=\".wrd\")\n",
    "    text_file = get_replace_ending(wav_path, new_extension=\".txt\")\n",
    "\n",
    "    example[\"phonetic_detail\"] = get_transcription_detail(phn_file)\n",
    "    example[\"word_detail\"] = get_transcription_detail(wrd_file)\n",
    "    example[\"text\"] = get_text(text_file)\n",
    "\n",
    "    # Speech Duration\n",
    "    example[\"duration\"] = get_speech_duration(\n",
    "        example[\"audio\"][\"array\"], sr=example[\"audio\"][\"sampling_rate\"])\n",
    "\n",
    "    # TIMIT Path\n",
    "    example[\"timit_path\"] = get_timit_path(\n",
    "        abs_path=wav_path, base_path=PATH_TO_TIMIT)\n",
    "\n",
    "    # Speaker Metadata\n",
    "    (dialect_region, dialect_region_name), (speaker_id, sex) = get_speaker_info(\n",
    "        example[\"timit_path\"])\n",
    "    example[\"dialect_region\"] = dialect_region\n",
    "    example[\"dialect_region_name\"] = dialect_region_name\n",
    "    example[\"speaker_id\"] = speaker_id\n",
    "    example[\"speaker_sex\"] = sex\n",
    "\n",
    "    # Sentence Metadata\n",
    "    sentence_id, sentence_type = get_sentence_info(wav_path)\n",
    "    example[\"id\"] = sentence_id\n",
    "    example[\"sentence_type\"] = sentence_type\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_with_metadata = train_dataset.map(add_metadata)\n",
    "testvalidation_dataset_with_metadata = testvalidation_dataset.map(add_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Upload Dataset to HuggingFace Hub\n",
    "- Note: we will upload the `testvalidation_dataset_with_metadata` as `test` in the TIMIT tradition\n",
    "- We will also create a train/validation/test split and have more features extracted from the data but do so in a different repository to remain consistent with the original TIMIT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_asr_dataset_base = DatasetDict({\n",
    "    \"train\": train_dataset_with_metadata,\n",
    "    \"test\": testvalidation_dataset_with_metadata\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_asr_dataset_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push Upload and Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_asr_dataset_base.push_to_hub(UPLOAD_TIMIT_BASE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_asr_dataset_base_from_hub = load_dataset(UPLOAD_TIMIT_BASE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the repository is pulled down correctly\n",
    "print(timit_asr_dataset_base_from_hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) Adding Phonetic Transcriptions\n",
    "- Although the TIMIT IPA has a `phonetic_detail` which is documented in `TIMIT/DOC/PHONCODE.DOC`, some downstream use cases may require the use of transcription records in the International Phonetic Alphabet (IPA) format\n",
    "- This format is universal accross almost every tradition in phonetics \n",
    "- Note: the transcription scheme used in the repository below is not peer reviewed and is specific to a broad transcription for evaluating a fine tuned Wav2Vec2 model against Wav2Vec2-XLSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ipa_transcription(example):\n",
    "    \"\"\"\n",
    "    Returns an interpretation of the IPA transcription in the `ipa_transcription` property\n",
    "    \"\"\"\n",
    "    example_with_metadata = add_metadata(example)\n",
    "    example_with_metadata[\"ipa_transcription\"] = get_ipa_transcription(example_with_metadata[\"phonetic_detail\"])\n",
    "    return example_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_with_ipa = train_dataset.map(add_ipa_transcription)\n",
    "testvalidation_dataset_with_ipa = testvalidation_dataset.map(add_ipa_transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check the transcriptions\n",
    "print(train_dataset_with_ipa[0][\"text\"])\n",
    "print([seg[\"utterance\"] for seg in train_dataset_with_ipa[0][\"phonetic_detail\"]])\n",
    "print(train_dataset_with_ipa[0][\"ipa_transcription\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check the transcriptions\n",
    "print(testvalidation_dataset_with_ipa[0][\"text\"])\n",
    "print([seg[\"utterance\"] for seg in testvalidation_dataset_with_ipa[0][\"phonetic_detail\"]])\n",
    "print(testvalidation_dataset_with_ipa[0][\"ipa_transcription\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5) Add Validation Split\n",
    "- The original TIMIT dataset has only `TRAIN` and `TEST`\n",
    "- Thus, we will create an 80-10-10 `TRAIN`/ `VALIDATION` / `TEST`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratification\n",
    "- The TIMIT dataset is carefully split into a `TRAIN` and `TEST` dataset.\n",
    "- When making the Validation dataset from splitting the `TEST` dataset in half, we want to `stratify` or keep the proportion of speakers the same to ensure accurate hyperparameter tuning and final test evaluation\n",
    "- The parameters that the TIMIT dataset stratifies on for `TRAIN` and `TEST` is `sex` and `dialect_region`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset, test_dataset = stratify_timt_dataset(\n",
    "    testvalidation_dataset_with_ipa\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that the dialect regions are balanced in validation set\n",
    "validation_dataset.to_pandas()[\"dialect_region_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that the dialect regions are balanced in test set\n",
    "test_dataset.to_pandas()[\"dialect_region_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6) Upload IPA and Validation Test split to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_ipa_dataset = DatasetDict({\n",
    "    \"train\": train_dataset_with_ipa,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_ipa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_ipa_dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_ipa_dataset[\"test\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_ipa_dataset.push_to_hub(UPLOAD_TIMIT_IPA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that upload succeeded\n",
    "timit_ipa_from_hub = load_dataset(UPLOAD_TIMIT_IPA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_ipa_from_hub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
